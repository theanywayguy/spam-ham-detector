{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Function to import the spam-ham dataset"
      ],
      "metadata": {
        "id": "wQjfAe83GMOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "XdYr4EEQkCd5"
      },
      "outputs": [],
      "source": [
        "import tarfile  # to handle tar archives (.tar, .tar.bz2)\n",
        "from pathlib import Path  # for filesystem path manipulation\n",
        "import urllib.request  # to download files from URLs\n",
        "\n",
        "def fetch_spam_data():\n",
        "    spam_root = \"http://spamassassin.apache.org/old/publiccorpus/\"  # base URL for dataset\n",
        "    ham_url = spam_root + \"20030228_easy_ham.tar.bz2\"  # URL for ham emails\n",
        "    spam_url = spam_root + \"20030228_spam.tar.bz2\"     # URL for spam emails\n",
        "\n",
        "    spam_path = Path() / \"datasets\" / \"spam\"  # local folder to store downloaded datasets\n",
        "    spam_path.mkdir(parents=True, exist_ok=True)  # create folder if it doesn't exist\n",
        "\n",
        "    # iterate over ham and spam datasets\n",
        "    for dir_name, tar_name, url in ((\"easy_ham\", \"ham\", ham_url),\n",
        "                                    (\"spam\", \"spam\", spam_url)):\n",
        "        if not (spam_path / dir_name).is_dir():  # check if dataset already extracted\n",
        "            path = (spam_path / tar_name).with_suffix(\".tar.bz2\")  # local tar.bz2 filename\n",
        "            print(\"Downloading\", path)  # notify user\n",
        "            urllib.request.urlretrieve(url, path)  # download the tar.bz2 file\n",
        "            tar_bz2_file = tarfile.open(path)  # open tar.bz2 archive\n",
        "            tar_bz2_file.extractall(path=spam_path)  # extract all contents to target folder\n",
        "            tar_bz2_file.close()  # close archive\n",
        "\n",
        "    # return paths to extracted ham and spam folders\n",
        "    return [spam_path / dir_name for dir_name in (\"easy_ham\", \"spam\")]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ham_dir, spam_dir = fetch_spam_data()"
      ],
      "metadata": {
        "id": "3O5s7jJfknnb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the ham directory, sorted alphabetically, filter out short filenames (<21 chars)\n",
        "ham_filenames = [f for f in sorted(ham_dir.iterdir()) if len(f.name) > 20]\n",
        "\n",
        "# List all files in the spam directory, sorted alphabetically, filter out short filenames (<21 chars)\n",
        "spam_filenames = [f for f in sorted(spam_dir.iterdir()) if len(f.name) > 20]\n",
        "\n",
        "# Print number of ham emails found\n",
        "print(len(ham_filenames))\n",
        "\n",
        "# Print number of spam emails found\n",
        "print(len(spam_filenames))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDtjAVfblf8i",
        "outputId": "5d75ba57-4fa4-45fa-851f-7e1558de0ac9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import email  # for parsing email messages\n",
        "import email.policy  # to define parsing behavior (e.g., preserving headers, line endings)\n",
        "\n",
        "# Function to load an email from a file path\n",
        "def load_email(filepath):\n",
        "    with open(filepath, \"rb\") as f:  # open file in binary mode\n",
        "        # parse the email using the default policy, return EmailMessage object\n",
        "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
        "\n",
        "# Load all ham emails into a list of EmailMessage objects\n",
        "ham_emails = [load_email(filepath) for filepath in ham_filenames]\n",
        "\n",
        "# Load all spam emails into a list of EmailMessage objects\n",
        "spam_emails = [load_email(filepath) for filepath in spam_filenames]\n",
        "\n",
        "# Print the content of the second ham email, stripped of leading/trailing whitespace\n",
        "print(ham_emails[1].get_content().strip())\n",
        "\n",
        "# Print the content of the seventh spam email, stripped of leading/trailing whitespace\n",
        "print(spam_emails[6].get_content().strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtbblPFSlwBl",
        "outputId": "288de24f-01c9-4a7a-9fbb-ad848f4760b7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Martin A posted:\n",
            "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
            " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
            " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
            " \n",
            " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
            " museum, a restored amphitheatre and car park for admiring crowds are\n",
            "planned\n",
            "---------------------\n",
            "So is this mountain limestone or granite?\n",
            "If it's limestone, it'll weather pretty fast.\n",
            "\n",
            "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
            "4 DVDs Free +s&p Join Now\n",
            "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
            "---------------------------------------------------------------------~->\n",
            "\n",
            "To unsubscribe from this group, send an email to:\n",
            "forteana-unsubscribe@egroups.com\n",
            "\n",
            " \n",
            "\n",
            "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n",
            "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
            "growing at a tremendous rate.  We are looking for individuals who\n",
            "want to work from home.\n",
            "\n",
            "This is an opportunity to make an excellent income.  No experience\n",
            "is required.  We will train you.\n",
            "\n",
            "So if you are looking to be employed from home with a career that has\n",
            "vast opportunities, then go:\n",
            "\n",
            "http://www.basetel.com/wealthnow\n",
            "\n",
            "We are looking for energetic and self motivated people.  If that is you\n",
            "than click on the link and fill out the form, and one of our\n",
            "employement specialist will contact you.\n",
            "\n",
            "To be removed from our link simple go to:\n",
            "\n",
            "http://www.basetel.com/remove.html\n",
            "\n",
            "\n",
            "4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to recursively describe the structure of an email\n",
        "def get_email_structure(email):\n",
        "    if isinstance(email, str):  # if input is already a string, return it\n",
        "        return email\n",
        "    payload = email.get_payload()  # get the payload (content) of the email\n",
        "    if isinstance(payload, list):  # if payload is a list, it's multipart\n",
        "        # recursively get structure of each part and join with commas\n",
        "        multipart = \", \".join([get_email_structure(sub_email)\n",
        "                               for sub_email in payload])\n",
        "        return f\"multipart({multipart})\"  # label as multipart with its parts\n",
        "    else:\n",
        "        return email.get_content_type()  # return content type (e.g., text/plain)\n"
      ],
      "metadata": {
        "id": "pQJE5mvpmBZz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter  # for counting occurrences\n",
        "\n",
        "# Function to count how many times each email structure appears\n",
        "def structures_counter(emails):\n",
        "    structures = Counter()  # initialize counter\n",
        "    for email in emails:\n",
        "        structure = get_email_structure(email)  # get structure of email\n",
        "        structures[structure] += 1  # increment count\n",
        "    return structures\n",
        "\n",
        "# Print the most common email structures in ham emails\n",
        "print(\n",
        "\"Common ham structure:\",\n",
        "structures_counter(ham_emails).most_common())\n",
        "\n",
        "# Print the most common email structures in spam emails\n",
        "print(\n",
        "\"Common spam structure:\",\n",
        "structures_counter(spam_emails).most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn253YIPHKZF",
        "outputId": "c629c537-bfc2-4bc6-81b5-9e0c4c5aa532"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common ham structure: [('text/plain', 2408), ('multipart(text/plain, application/pgp-signature)', 66), ('multipart(text/plain, text/html)', 8), ('multipart(text/plain, text/plain)', 4), ('multipart(text/plain)', 3), ('multipart(text/plain, application/octet-stream)', 2), ('multipart(text/plain, text/enriched)', 1), ('multipart(text/plain, application/ms-tnef, text/plain)', 1), ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)', 1), ('multipart(text/plain, video/mng)', 1), ('multipart(text/plain, multipart(text/plain))', 1), ('multipart(text/plain, application/x-pkcs7-signature)', 1), ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)', 1), ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))', 1), ('multipart(text/plain, application/x-java-applet)', 1)]\n",
            "Common spam structure: [('text/plain', 218), ('text/html', 183), ('multipart(text/plain, text/html)', 45), ('multipart(text/html)', 20), ('multipart(text/plain)', 19), ('multipart(multipart(text/html))', 5), ('multipart(text/plain, image/jpeg)', 3), ('multipart(text/html, application/octet-stream)', 2), ('multipart(text/plain, application/octet-stream)', 1), ('multipart(text/html, text/plain)', 1), ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1), ('multipart(multipart(text/plain, text/html), image/gif)', 1), ('multipart/alternative', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for header, value in spam_emails[0].items():\n",
        "    print(header, \":\", value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rPtsjUomSjv",
        "outputId": "04e46e53-ce19-4592-9897-7619cf3d5d64"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return-Path : <12a1mailbot1@web.de>\n",
            "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
            "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
            "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
            "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
            "From : 12a1mailbot1@web.de\n",
            "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
            "To : dcek1a1@netsgo.com\n",
            "Subject : Life Insurance - Why Pay More?\n",
            "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
            "MIME-Version : 1.0\n",
            "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
            "Content-Type : text/html; charset=\"iso-8859-1\"\n",
            "Content-Transfer-Encoding : quoted-printable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spam_emails[0][\"Subject\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nfOYLHPqmrDK",
        "outputId": "beba5e09-b805-4585-e46e-d389756d81c7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Life Insurance - Why Pay More?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML-Section:"
      ],
      "metadata": {
        "id": "36qI8v1mmwtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine ham and spam emails into a single numpy array, dtype=object to hold EmailMessage objects\n",
        "X = np.array(ham_emails + spam_emails, dtype=object)\n",
        "\n",
        "# Create labels: 0 for ham, 1 for spam\n",
        "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "# random_state=42 ensures reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)\n"
      ],
      "metadata": {
        "id": "yHvJNft5m05h"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup  # for parsing HTML content\n",
        "from html import unescape      # to convert HTML entities to normal text\n",
        "import re                      # for regular expressions\n",
        "\n",
        "# Convert HTML content of an email to plain text\n",
        "def html_to_plain_text(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")  # parse HTML\n",
        "    # remove <head> section and its contents, often contains metadata/scripts\n",
        "    if soup.head:\n",
        "        soup.head.decompose()\n",
        "    # replace all hyperlinks with the word 'HYPERLINK'\n",
        "    for a_tag in soup.find_all(\"a\"):\n",
        "        a_tag.replace_with(\" HYPERLINK \")\n",
        "    # extract visible text, using newline as separator\n",
        "    text = soup.get_text(separator=\"\\n\")\n",
        "    # normalize multiple consecutive newlines into a single newline\n",
        "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M)\n",
        "    return unescape(text)  # convert HTML entities (e.g., &amp;) to characters\n",
        "\n"
      ],
      "metadata": {
        "id": "wsoWBryDnFXv"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html_spam_emails = [email for email in X_train[y_train==1]\n",
        "                    if get_email_structure(email) == \"text/html\"]\n",
        "sample_html_spam = html_spam_emails[7]\n",
        "print(sample_html_spam.get_content().strip()[:1000], \"...\")\n",
        "\n",
        "\n",
        "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2azS1IzdnI_y",
        "outputId": "b1af2473-3811-4b06-b016-8f4dee6af751"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<HTML><HEAD><TITLE></TITLE><META http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\"><STYLE>A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}</STYLE><META content=\"MSHTML 6.00.2713.1100\" name=\"GENERATOR\"></HEAD>\n",
            "<BODY text=\"#000000\" vLink=\"#0033ff\" link=\"#0033ff\" bgColor=\"#CCCC99\"><TABLE borderColor=\"#660000\" cellSpacing=\"0\" cellPadding=\"0\" border=\"0\" width=\"100%\"><TR><TD bgColor=\"#CCCC99\" valign=\"top\" colspan=\"2\" height=\"27\">\n",
            "<font size=\"6\" face=\"Arial, Helvetica, sans-serif\" color=\"#660000\">\n",
            "<b>OTC</b></font></TD></TR><TR><TD height=\"2\" bgcolor=\"#6a694f\">\n",
            "<font size=\"5\" face=\"Times New Roman, Times, serif\" color=\"#FFFFFF\">\n",
            "<b>&nbsp;Newsletter</b></font></TD><TD height=\"2\" bgcolor=\"#6a694f\"><div align=\"right\"><font color=\"#FFFFFF\">\n",
            "<b>Discover Tomorrow's Winners&nbsp;</b></font></div></TD></TR><TR><TD height=\"25\" colspan=\"2\" bgcolor=\"#CCCC99\"><table width=\"100%\" border=\"0\"  ...\n",
            "\n",
            "OTC\n",
            " Newsletter\n",
            "Discover Tomorrow's Winners\n",
            "For Immediate Release\n",
            "Cal-Bay (Stock Symbol: CBYI)\n",
            "Watch for analyst \"Strong Buy Recommendations\" and several advisory newsletters picking CBYI.  CBYI has filed to be traded on the OTCBB, share prices historically INCREASE when companies get listed on this larger trading exchange. CBYI is trading around 25 cents and should skyrocket to $2.66 - $3.25 a share in the near future.\n",
            "Put CBYI on your watch list, acquire a position TODAY.\n",
            "REASONS TO INVEST IN CBYI\n",
            "A profitable company and is on track to beat ALL earnings estimates!\n",
            "One of the FASTEST growing distributors in environmental & safety equipment instruments.\n",
            "Excellent management team, several EXCLUSIVE contracts.  IMPRESSIVE client list including the U.S. Air Force, Anheuser-Busch, Chevron Refining and Mitsubishi Heavy Industries, GE-Energy & Environmental Research.\n",
            "RAPIDLY GROWING INDUSTRY\n",
            "Industry revenues exceed $900 million, estimates indicate that there could be as much as $25 billio ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def email_to_text(email):\n",
        "    html = None\n",
        "    for part in email.walk():\n",
        "        ctype = part.get_content_type()\n",
        "        if not ctype in (\"text/plain\", \"text/html\"):\n",
        "            continue\n",
        "        try:\n",
        "            content = part.get_content()\n",
        "        except: # in case of encoding issues\n",
        "            content = str(part.get_payload())\n",
        "        if ctype == \"text/plain\":\n",
        "            return content\n",
        "        else:\n",
        "            html = content\n",
        "    if html:\n",
        "        return html_to_plain_text(html)\n",
        "\n",
        "\n",
        "\n",
        "print(email_to_text(sample_html_spam)[:100], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBuYxGmZn3mQ",
        "outputId": "3019290b-d672-42a6-ccb0-dca4182cae94"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OTC\n",
            " Newsletter\n",
            "Discover Tomorrow's Winners\n",
            "For Immediate Release\n",
            "Cal-Bay (Stock Symbol: CBYI)\n",
            "Watc ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk  # Natural Language Toolkit for text processing\n",
        "\n",
        "# Initialize the Porter stemming algorithm\n",
        "stemmer = nltk.PorterStemmer()\n",
        "\n",
        "# Test stemming on different forms of the word \"compute\"\n",
        "for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\",\n",
        "             \"Compulsive\"):\n",
        "    # stem() reduces words to their root form\n",
        "    print(word, \"=>\", stemmer.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBXqkyBbFdCM",
        "outputId": "f50d5922-0a0b-4c7e-cfa5-932744b4da62"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computations => comput\n",
            "Computation => comput\n",
            "Computing => comput\n",
            "Computed => comput\n",
            "Compute => comput\n",
            "Compulsive => compuls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "# if running this notebook on Colab or Kaggle, we just pip install urlextract\n",
        "if IS_COLAB or IS_KAGGLE:\n",
        "    %pip install -q -U urlextract"
      ],
      "metadata": {
        "id": "YENelMIqoLMA"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urlextract  # library to extract URLs from text (may download root domain names)\n",
        "\n",
        "# Create an instance of the URL extractor\n",
        "url_extractor = urlextract.URLExtract()\n",
        "\n",
        "# Example text containing URLs\n",
        "some_text = \"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"\n",
        "\n",
        "# Extract and print all URLs found in the text\n",
        "print(url_extractor.find_urls(some_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q24ezBbBoUCY",
        "outputId": "a7e3fb26-8e58-4ddc-9c63-bec5970b0db2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin  # base classes for custom transformers\n",
        "\n",
        "# Transformer that converts emails into word count dictionaries\n",
        "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strip_headers=True, lower_case=True,\n",
        "                 remove_punctuation=True, replace_urls=True,\n",
        "                 replace_numbers=True, stemming=True):\n",
        "        # configuration options for preprocessing\n",
        "        self.strip_headers = strip_headers\n",
        "        self.lower_case = lower_case\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.replace_urls = replace_urls\n",
        "        self.replace_numbers = replace_numbers\n",
        "        self.stemming = stemming\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # no fitting needed, just a transformer\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_transformed = []  # store transformed emails\n",
        "        for email in X:\n",
        "            text = email_to_text(email) or \"\"  # extract email text\n",
        "            if self.lower_case:\n",
        "                text = text.lower()  # convert to lowercase\n",
        "            if self.replace_urls and url_extractor is not None:\n",
        "                # find all URLs, sort by length descending to avoid partial replacements\n",
        "                urls = list(set(url_extractor.find_urls(text)))\n",
        "                urls.sort(key=lambda url: len(url), reverse=True)\n",
        "                for url in urls:\n",
        "                    text = text.replace(url, \" URL \")  # replace URLs with placeholder\n",
        "            if self.replace_numbers:\n",
        "                # replace all numeric patterns with placeholder\n",
        "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
        "            if self.remove_punctuation:\n",
        "                # remove all non-word characters\n",
        "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
        "            word_counts = Counter(text.split())  # count word frequencies\n",
        "            if self.stemming and stemmer is not None:\n",
        "                # reduce words to their stems\n",
        "                stemmed_word_counts = Counter()\n",
        "                for word, count in word_counts.items():\n",
        "                    stemmed_word = stemmer.stem(word)\n",
        "                    stemmed_word_counts[stemmed_word] += count\n",
        "                word_counts = stemmed_word_counts\n",
        "            X_transformed.append(word_counts)  # add processed email word counts\n",
        "        return np.array(X_transformed)  # return as NumPy array\n"
      ],
      "metadata": {
        "id": "qivQEbTWpSaO"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_few = X_train[:3]\n",
        "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
        "X_few_wordcounts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6At9KG-NE0DL",
        "outputId": "f8e52d52-4356-4d6f-fee4-bb6736df5a4c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
              "       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n",
              "       Counter({'url': 4, 's': 3, 'group': 3, 'to': 3, 'in': 2, 'forteana': 2, 'martin': 2, 'an': 2, 'and': 2, 'we': 2, 'is': 2, 'yahoo': 2, 'unsubscrib': 2, 'y': 1, 'adamson': 1, 'wrote': 1, 'for': 1, 'altern': 1, 'rather': 1, 'more': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'on': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'hi': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright': 1, 'know': 1, 'how': 1, 'unbias': 1, 'memri': 1, 'don': 1, 't': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'number': 1, 'dvd': 1, 'free': 1, 'p': 1, 'join': 1, 'now': 1, 'from': 1, 'thi': 1, 'send': 1, 'email': 1, 'egroup': 1, 'com': 1, 'your': 1, 'use': 1, 'of': 1, 'subject': 1})],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix  # for sparse matrix representation\n",
        "\n",
        "# Transformer that converts word count dictionaries into vectors\n",
        "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, vocabulary_size=1000):\n",
        "        self.vocabulary_size = vocabulary_size  # maximum number of words to keep in vocabulary\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        total_count = Counter()  # count total occurrences across all emails\n",
        "        for word_count in X:\n",
        "            for word, count in word_count.items():\n",
        "                # limit each word's contribution to 10 to avoid dominance\n",
        "                total_count[word] += min(count, 10)\n",
        "        # select most common words up to vocabulary_size\n",
        "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
        "        # assign an index to each word\n",
        "        self.vocabulary_ = {word: index + 1\n",
        "                            for index, (word, count) in enumerate(most_common)}\n",
        "        print(\"Vocabulary built:\", self.vocabulary_)  # print vocabulary mapping\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        rows = []\n",
        "        cols = []\n",
        "        data = []\n",
        "        for row, word_count in enumerate(X):\n",
        "            for word, count in word_count.items():\n",
        "                rows.append(row)\n",
        "                cols.append(self.vocabulary_.get(word, 0))  # 0 if word not in vocabulary\n",
        "                data.append(count)\n",
        "        sparse_matrix = csr_matrix((data, (rows, cols)),\n",
        "                                   shape=(len(X), self.vocabulary_size + 1))\n",
        "        print(\"Sparse matrix shape:\", sparse_matrix.shape)  # print matrix shape\n",
        "        return sparse_matrix\n",
        "\n",
        "# Example usage with small dataset\n",
        "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
        "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
        "print(X_few_vectors)  # print resulting sparse vectors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtofrNZoE2MZ",
        "outputId": "2bef48db-0487-4bac-9449-797a9398d1dd"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary built: {'the': 1, 'of': 2, 'and': 3, 'to': 4, 'url': 5, 'all': 6, 'in': 7, 'christian': 8, 'on': 9, 'by': 10}\n",
            "Sparse matrix shape: (3, 11)\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 20 stored elements and shape (3, 11)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t6\n",
            "  (1, 0)\t99\n",
            "  (1, 1)\t11\n",
            "  (1, 2)\t9\n",
            "  (1, 3)\t8\n",
            "  (1, 4)\t3\n",
            "  (1, 5)\t1\n",
            "  (1, 6)\t3\n",
            "  (1, 7)\t1\n",
            "  (1, 8)\t3\n",
            "  (1, 9)\t2\n",
            "  (1, 10)\t3\n",
            "  (2, 0)\t67\n",
            "  (2, 2)\t1\n",
            "  (2, 3)\t2\n",
            "  (2, 4)\t3\n",
            "  (2, 5)\t4\n",
            "  (2, 6)\t1\n",
            "  (2, 7)\t2\n",
            "  (2, 9)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_transformer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY81L8doE83a",
        "outputId": "8fc986fd-e5fe-420b-ffcb-344e5802febc"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'of': 2,\n",
              " 'and': 3,\n",
              " 'to': 4,\n",
              " 'url': 5,\n",
              " 'all': 6,\n",
              " 'in': 7,\n",
              " 'christian': 8,\n",
              " 'on': 9,\n",
              " 'by': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline  # for chaining multiple preprocessing steps\n",
        "\n",
        "# Create a preprocessing pipeline:\n",
        "# 1. Convert emails to word count dictionaries\n",
        "# 2. Convert word counts to fixed-size vectors\n",
        "preprocess_pipeline = Pipeline([\n",
        "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
        "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
        "])\n",
        "\n",
        "# Fit the pipeline on training data and transform it into numerical vectors\n",
        "X_train_transformed = preprocess_pipeline.fit_transform(X_train)\n",
        "print(\"Transformed training data shape:\", X_train_transformed.shape)  # print shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHFqXHtII5EY",
        "outputId": "c54fe44a-cb53-4941-937e-0f3ce73983d0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary built: {'number': 1, 'the': 2, 'to': 3, 'a': 4, 'and': 5, 'of': 6, 'i': 7, 'in': 8, 'it': 9, 'url': 10, 'is': 11, 'that': 12, 'you': 13, 'for': 14, 'thi': 15, 'on': 16, 's': 17, 'be': 18, 'with': 19, 'have': 20, 'from': 21, 'not': 22, 'are': 23, 't': 24, 'as': 25, 'or': 26, 'your': 27, 'list': 28, 'if': 29, 'but': 30, 'at': 31, 'use': 32, 'can': 33, 'by': 34, 'all': 35, 'an': 36, 'my': 37, 'wa': 38, 'we': 39, 'get': 40, 'mail': 41, 'do': 42, 'they': 43, 'will': 44, 'so': 45, 'one': 46, 'there': 47, 'more': 48, 'ha': 49, 'time': 50, 'just': 51, 'about': 52, 'no': 53, 'what': 54, 'out': 55, 'like': 56, 'messag': 57, 'com': 58, 'up': 59, 'email': 60, 'onli': 61, 'which': 62, 'would': 63, 'work': 64, 'other': 65, 'make': 66, 'don': 67, 'some': 68, 'who': 69, 'ani': 70, 'me': 71, 'now': 72, 'when': 73, 'new': 74, 'peopl': 75, 'their': 76, 'our': 77, 'm': 78, 'free': 79, 'user': 80, 'been': 81, 'net': 82, 'date': 83, 'wrote': 84, 'how': 85, 'want': 86, 'than': 87, 'rpm': 88, 'them': 89, 'linux': 90, 'then': 91, 'also': 92, 'spamassassin': 93, 'file': 94, 'need': 95, 'he': 96, 'go': 97, 'year': 98, 'way': 99, 'think': 100, 'here': 101, 're': 102, 'becaus': 103, 'group': 104, 'into': 105, 'tri': 106, 'hi': 107, 'look': 108, 'know': 109, 'receiv': 110, 'should': 111, 'us': 112, 'had': 113, '_______________________________________________': 114, 'over': 115, 'run': 116, 'hyperlink': 117, 'e': 118, 'inform': 119, 've': 120, 'even': 121, 'd': 122, 'spam': 123, 'said': 124, 'could': 125, 'first': 126, 'most': 127, 'say': 128, 'see': 129, 'right': 130, 'chang': 131, 'problem': 132, 'were': 133, 'exmh': 134, 'pleas': 135, 'these': 136, 'world': 137, 'line': 138, 'day': 139, 'thing': 140, 'take': 141, 'may': 142, 'll': 143, 'money': 144, 'send': 145, 'system': 146, 'same': 147, 'where': 148, 'find': 149, 'good': 150, 'remov': 151, 'after': 152, 'much': 153, 'well': 154, 'name': 155, 'doe': 156, 'compani': 157, 'call': 158, 'those': 159, 'mani': 160, 'help': 161, 'veri': 162, 'talk': 163, 'servic': 164, 'subject': 165, 'am': 166, 'packag': 167, 'address': 168, 'includ': 169, 'start': 170, 'busi': 171, 'internet': 172, 'befor': 173, 'give': 174, 'state': 175, 'link': 176, 'set': 177, 'home': 178, 'own': 179, 'instal': 180, 'still': 181, 'someth': 182, 'through': 183, 'two': 184, 'come': 185, 'best': 186, 'gener': 187, 'differ': 188, 'too': 189, 'such': 190, 'realli': 191, 'site': 192, 'market': 193, 'seem': 194, 'phone': 195, 'write': 196, 'web': 197, 'comput': 198, 'c': 199, 'data': 200, 'unsubscrib': 201, 'softwar': 202, 'program': 203, 'read': 204, 'few': 205, 'person': 206, 'point': 207, 'check': 208, 'ie': 209, 'interest': 210, 'sure': 211, 'old': 212, 'found': 213, 'back': 214, 'base': 215, 'sep': 216, 'whi': 217, 'off': 218, 'report': 219, 'made': 220, 'million': 221, 'everi': 222, 'sponsor': 223, 'offer': 224, 'sinc': 225, 'each': 226, 'current': 227, 'last': 228, 'did': 229, 'server': 230, 'order': 231, 'case': 232, 'numbertnumb': 233, 'actual': 234, 'provid': 235, 'end': 236, 'today': 237, 'commun': 238, 'fork': 239, 'down': 240, 'follow': 241, 'secur': 242, 'origin': 243, 'mean': 244, 'next': 245, 'doesn': 246, 'test': 247, 'code': 248, 'anoth': 249, 'got': 250, 'perl': 251, 'r': 252, 'without': 253, 'long': 254, 'never': 255, 'put': 256, 'better': 257, 'razor': 258, 'anyon': 259, 'live': 260, 'real': 261, 'thank': 262, 'lot': 263, 'part': 264, 'place': 265, 'might': 266, 'sent': 267, 'manag': 268, 'onc': 269, 'month': 270, 'sourceforg': 271, 'while': 272, 'again': 273, 'x': 274, 'keep': 275, 'folder': 276, 'idea': 277, 'life': 278, 'ad': 279, 'network': 280, 'build': 281, 'error': 282, 'form': 283, 'week': 284, 'within': 285, 'version': 286, 'type': 287, 'requir': 288, 'invest': 289, 'probabl': 290, 'govern': 291, 'countri': 292, 'product': 293, 'show': 294, 'rate': 295, 'org': 296, 'yahoo': 297, 'support': 298, 'post': 299, 'issu': 300, 'discuss': 301, 'someon': 302, 'price': 303, 'click': 304, 'window': 305, 'mailto': 306, 'reason': 307, 'both': 308, 'sourc': 309, 'high': 310, 'releas': 311, 'suppli': 312, 'word': 313, 'custom': 314, 'develop': 315, 'easi': 316, 'must': 317, 'move': 318, 'redhat': 319, 'page': 320, 'let': 321, 'anyth': 322, 'result': 323, 'text': 324, 'etc': 325, 'less': 326, 'freshrpm': 327, 'past': 328, 'plan': 329, 'didn': 330, 'septemb': 331, 'around': 332, 'per': 333, 'content': 334, 'she': 335, 'news': 336, 'between': 337, 'total': 338, 'exampl': 339, 'train': 340, 'done': 341, 'ye': 342, 'allow': 343, 'great': 344, 'key': 345, 'f': 346, 'share': 347, 'though': 348, 'pay': 349, 'public': 350, 'wish': 351, 'tell': 352, 'matthia': 353, 'access': 354, 'maintain': 355, 'cours': 356, 'add': 357, 'sell': 358, 'hour': 359, 'happen': 360, 'her': 361, 'save': 362, 'kernel': 363, 'avail': 364, 'question': 365, 'open': 366, 'possibl': 367, 'ever': 368, 'under': 369, 'engin': 370, 'html': 371, 'log': 372, 'least': 373, 'instead': 374, 'power': 375, 'els': 376, 'header': 377, 'second': 378, 'red': 379, 'below': 380, 'littl': 381, 'buy': 382, 'account': 383, 'ask': 384, 'prefer': 385, 'believ': 386, 'creat': 387, 'rule': 388, 'stuff': 389, 'experi': 390, 'applic': 391, 'exist': 392, 'polit': 393, 'local': 394, 'recent': 395, 'futur': 396, 'updat': 397, 'success': 398, 'american': 399, 'process': 400, 'law': 401, 'mayb': 402, 'complet': 403, 'un': 404, 'friend': 405, 'import': 406, 'yet': 407, 'technolog': 408, 'fact': 409, 'enough': 410, 'against': 411, 'special': 412, 'howev': 413, 'w': 414, 'b': 415, 'thought': 416, 'bit': 417, 'contact': 418, 'deal': 419, 'becom': 420, 'l': 421, 'pm': 422, 'noth': 423, 'u': 424, 'stori': 425, 'select': 426, 'search': 427, 'big': 428, 'featur': 429, 'hand': 430, 'subscript': 431, 'far': 432, 'industri': 433, 'valu': 434, 'ham': 435, 'non': 436, 'larg': 437, 'alway': 438, 'client': 439, 'apt': 440, 'hat': 441, 'term': 442, 'alsa': 443, 'control': 444, 'offic': 445, 'p': 446, 'kind': 447, 'alreadi': 448, 'dollar': 449, 'thu': 450, 'learn': 451, 'cost': 452, 'trade': 453, 'protect': 454, 'onlin': 455, 'note': 456, 'stop': 457, 'isn': 458, 'play': 459, 'turn': 460, 'irish': 461, 'forward': 462, 'him': 463, 'n': 464, 'admin': 465, 'nation': 466, 'design': 467, 'thousand': 468, 'citi': 469, 'won': 470, 'book': 471, 'copi': 472, 'simpl': 473, 'understand': 474, 'hit': 475, 'feel': 476, 'love': 477, 'cd': 478, 'advertis': 479, 'respons': 480, '_': 481, 'effect': 482, 'directori': 483, 'record': 484, 'fix': 485, 'man': 486, 'aug': 487, 'score': 488, 'financi': 489, 'sound': 490, 'opportun': 491, 'seen': 492, 'either': 493, 'job': 494, 'act': 495, 'load': 496, 'ilug': 497, 'unit': 498, 'famili': 499, 'root': 500, 'increas': 501, 'dvd': 502, 'three': 503, 'mr': 504, 'legal': 505, 'card': 506, 'incom': 507, 'fund': 508, 'away': 509, 'databas': 510, 'box': 511, 'sequenc': 512, 'limit': 513, 'hope': 514, 'usr': 515, 'hard': 516, 'member': 517, 'appear': 518, 'claim': 519, 'full': 520, 'begin': 521, 'ok': 522, 'forc': 523, 'top': 524, 'corpor': 525, 'return': 526, 'answer': 527, 'true': 528, 'expect': 529, 'posit': 530, 'standard': 531, 'mind': 532, 'repli': 533, 'presid': 534, 'step': 535, 'rather': 536, 'join': 537, 'wrong': 538, 'connect': 539, 'level': 540, 'abl': 541, 'caus': 542, 'until': 543, 'g': 544, 'often': 545, 'care': 546, 'small': 547, 'direct': 548, 'bill': 549, 'drive': 550, 'rememb': 551, 'listmast': 552, 'j': 553, 'major': 554, 'machin': 555, 'signatur': 556, 'oper': 557, 'mark': 558, 'cell': 559, 'quit': 560, 'o': 561, 'specif': 562, 'partner': 563, 'regard': 564, 'chri': 565, 'ago': 566, 'profession': 567, 'perform': 568, 'accept': 569, 'notic': 570, 'size': 571, 'war': 572, 'forteana': 573, 'wed': 574, 'depend': 575, 'individu': 576, 'game': 577, 'wonder': 578, 'continu': 579, 'cash': 580, 'whether': 581, 'center': 582, 'bank': 583, 'project': 584, 'everyth': 585, 'bad': 586, 'sever': 587, 'univers': 588, 'devel': 589, 'sort': 590, 'abov': 591, 'contain': 592, 'welcom': 593, 'guarante': 594, 'area': 595, 'risk': 596, 'given': 597, 'copyright': 598, 'figur': 599, 'immedi': 600, 'dure': 601, 'bug': 602, 'parti': 603, 'polici': 604, 'further': 605, 'subscrib': 606, 'singl': 607, 'wait': 608, 'lost': 609, 'assist': 610, 'xent': 611, 'upon': 612, 'clean': 613, 'grow': 614, 'geek': 615, 'women': 616, 'digit': 617, 'request': 618, 'script': 619, 'lib': 620, 'sign': 621, 'anyway': 622, 'simpli': 623, 'comment': 624, 'intern': 625, 'quot': 626, 'half': 627, 'fail': 628, 'pretti': 629, 'entir': 630, 'econom': 631, 'info': 632, 'america': 633, 'fals': 634, 'tool': 635, 'whole': 636, 'meet': 637, 'execut': 638, 'privat': 639, 'imag': 640, 'option': 641, 'view': 642, 'decid': 643, 'java': 644, 'nice': 645, 'pick': 646, 'command': 647, 'upgrad': 648, 'matter': 649, 'later': 650, 'transact': 651, 'document': 652, 'ship': 653, 'worker': 654, 'msg': 655, 'egroup': 656, 'almost': 657, 'tire': 658, 'guy': 659, 'august': 660, 'detail': 661, 'h': 662, 'went': 663, 'john': 664, 'space': 665, 'solut': 666, 'addit': 667, 'similar': 668, 'via': 669, 'stock': 670, 'method': 671, 'agre': 672, 'suggest': 673, 'practic': 674, 'store': 675, 'transfer': 676, 'came': 677, 'final': 678, 'visit': 679, 'amount': 680, 'benefit': 681, 'except': 682, 'school': 683, 'fax': 684, 'attack': 685, 'fast': 686, 'agent': 687, 'websit': 688, 'discov': 689, 'easili': 690, 'research': 691, 'articl': 692, 'relat': 693, 'low': 694, 'minut': 695, 'global': 696, 'societi': 697, 'regist': 698, 'author': 699, 'exactli': 700, 'languag': 701, 'told': 702, 'interact': 703, 'although': 704, 'close': 705, 'modul': 706, 'sex': 707, 'instruct': 708, 'happi': 709, 'respect': 710, 'social': 711, 'watch': 712, 'distribut': 713, 'collect': 714, 'activ': 715, 'consid': 716, 'opt': 717, 'late': 718, 'memori': 719, 'profit': 720, 'driver': 721, 'publish': 722, 'domain': 723, 'feder': 724, 'python': 725, 'sa': 726, 'pass': 727, 'cut': 728, 'street': 729, 'lead': 730, 'daili': 731, 'left': 732, 'secret': 733, 'due': 734, 'mon': 735, 'hundr': 736, 'present': 737, 'bodi': 738, 'fall': 739, 'filter': 740, 'head': 741, 'associ': 742, 'bush': 743, 'everyon': 744, 'cv': 745, 'guess': 746, 'heaven': 747, 'assum': 748, 'involv': 749, 'monday': 750, 'propos': 751, 'spammer': 752, 'known': 753, 'face': 754, 'effort': 755, 'de': 756, 'reach': 757, 'die': 758, 'token': 759, 'improv': 760, 'perhap': 761, 'procmail': 762, 'basic': 763, 'appli': 764, 'worth': 765, 'insur': 766, 'leav': 767, 'natur': 768, 'remain': 769, 'clear': 770, 'miss': 771, 'fine': 772, 'choic': 773, 'explain': 774, 'normal': 775, 'car': 776, 'enabl': 777, 'tim': 778, 'fill': 779, 'hous': 780, 'googl': 781, 'offici': 782, 'action': 783, 'absolut': 784, 'inc': 785, 'tax': 786, 'deliv': 787, 'suit': 788, 'octob': 789, 'human': 790, 'men': 791, 'target': 792, 'osdn': 793, 'os': 794, 'credit': 795, 'devic': 796, 'platform': 797, 'thinkgeek': 798, 'charg': 799, 'averag': 800, 'paper': 801, 'beberg': 802, 'plu': 803, 'age': 804, 'download': 805, 'robert': 806, 'spend': 807, 'statement': 808, 'taint': 809, 'administr': 810, 'fri': 811, 'saou': 812, 'trust': 813, 'hettinga': 814, 'usa': 815, 'sale': 816, 'purchas': 817, 'delet': 818, 'main': 819, 'numberpm': 820, 'self': 821, 'unseen': 822, 'print': 823, 'ignor': 824, 'altern': 825, 'press': 826, 'port': 827, 'five': 828, 'rah': 829, 'electron': 830, 'suppos': 831, 'goe': 832, 'across': 833, 'contract': 834, 'speed': 835, 'along': 836, 'commiss': 837, 'particular': 838, 'integr': 839, 'refer': 840, 'cannot': 841, 'wasn': 842, 'paid': 843, 'york': 844, 'kill': 845, 'usual': 846, 'oct': 847, 'skip': 848, 'dave': 849, 'tag': 850, 'side': 851, 'hold': 852, 'track': 853, 'sun': 854, 'newslett': 855, 'organ': 856, 'studi': 857, 'capit': 858, 'folk': 859, 'night': 860, 'latest': 861, 'edit': 862, 'media': 863, 'regul': 864, 'commit': 865, 'potenti': 866, 'default': 867, 'display': 868, 'soon': 869, 'cc': 870, 'break': 871, 'hardwar': 872, 'lawrenc': 873, 'bitbitch': 874, 'compil': 875, 'adam': 876, 'letter': 877, 'william': 878, 'warn': 879, 'reserv': 880, 'object': 881, 'mass': 882, 'speak': 883, 'behalf': 884, 'abil': 885, 'male': 886, 'six': 887, 'unless': 888, 'concern': 889, 'haven': 890, 'hear': 891, 'invok': 892, 'earn': 893, 'lose': 894, 'yourself': 895, 'sens': 896, 'model': 897, 'mention': 898, 'implement': 899, 'co': 900, 'pictur': 901, 'tie': 902, 'longer': 903, 'pgp': 904, 'whatev': 905, 'util': 906, 'short': 907, 'earli': 908, 'configur': 909, 'gari': 910, 'itself': 911, 'id': 912, 'replac': 913, 'especi': 914, 'spec': 915, 'father': 916, 'third': 917, 'kid': 918, 'function': 919, 'procedur': 920, 'variou': 921, 'k': 922, 'scienc': 923, 'heard': 924, 'q': 925, 'advantag': 926, 'wouldn': 927, 'switch': 928, 'guid': 929, 'depart': 930, 'speech': 931, 'recommend': 932, 'insid': 933, 'produc': 934, 'indic': 935, 'numberth': 936, 'grant': 937, 'oh': 938, 'predict': 939, 'win': 940, 'qualiti': 941, 'togeth': 942, 'toni': 943, 'definit': 944, 'music': 945, 'appar': 946, 'washington': 947, 'button': 948, 'locat': 949, 'neg': 950, 'safe': 951, 'numberk': 952, 'aren': 953, 'took': 954, 'tuesday': 955, 'pc': 956, 'approach': 957, 'readi': 958, 'xml': 959, 'handl': 960, 'independ': 961, 'count': 962, 'rest': 963, 'attempt': 964, 'coupl': 965, 'valuabl': 966, 'format': 967, 'built': 968, 'popul': 969, 'certainli': 970, 'taken': 971, 'accord': 972, 'educ': 973, 'review': 974, 'section': 975, 'histori': 976, 'clue': 977, 'white': 978, 'spamd': 979, 'argument': 980, 'common': 981, 'dear': 982, 'consum': 983, 'modifi': 984, 'tom': 985, 'polic': 986, 'death': 987, 'michael': 988, 'mode': 989, 'quick': 990, 'rh': 991, 'disk': 992, 'south': 993, 'young': 994, 'deserv': 995, 'murphi': 996, 'bring': 997, 'path': 998, 'movi': 999, 'air': 1000}\n",
            "Sparse matrix shape: (2400, 1001)\n",
            "Transformed training data shape: (2400, 1001)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression  # classifier\n",
        "from sklearn.model_selection import cross_val_score  # for cross-validation\n",
        "\n",
        "# Initialize logistic regression\n",
        "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Evaluate using 3-fold cross-validation\n",
        "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3)\n",
        "print(\"Cross-validation scores:\", score)\n",
        "print(\"Mean CV score:\", score.mean())  # average performance across folds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMn3Mx94E-ci",
        "outputId": "210ad816-42c9-4cfc-e67b-9912172aaba9"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.98375 0.985   0.98875]\n",
            "Mean CV score: 0.9858333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score  # metrics to evaluate classifier\n",
        "\n",
        "# Transform test emails into vectors using the same preprocessing pipeline\n",
        "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
        "print(\"Transformed test data shape:\", X_test_transformed.shape)  # print shape\n",
        "\n",
        "# Initialize logistic regression classifier\n",
        "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "# Train on the transformed training data\n",
        "log_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Predict labels for the test set\n",
        "y_pred = log_clf.predict(X_test_transformed)\n",
        "\n",
        "# Print precision (proportion of predicted spam that is correct)\n",
        "print(f\"Precision: {precision_score(y_test, y_pred):.2%}\")\n",
        "# Print recall (proportion of actual spam correctly identified)\n",
        "print(f\"Recall: {recall_score(y_test, y_pred):.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJrLhFbxFBEI",
        "outputId": "e8b5e18e-3e33-48d5-cedd-4625762dbe03"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparse matrix shape: (600, 1001)\n",
            "Transformed test data shape: (600, 1001)\n",
            "Precision: 96.88%\n",
            "Recall: 97.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Fit the pipeline and classifier\n",
        "preprocess_pipeline.fit(X_train)\n",
        "X_train_transformed = preprocess_pipeline.transform(X_train)\n",
        "\n",
        "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_clf.fit(X_train_transformed, y_train)\n",
        "\n",
        "# Save both objects together\n",
        "joblib.dump({\n",
        "    \"preprocess_pipeline\": preprocess_pipeline,\n",
        "    \"classifier\": log_clf\n",
        "}, \"spam_classifier.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YG-XkbVMYkt",
        "outputId": "7ec1995e-eb77-498a-816f-294c39e2ac6b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary built: {'number': 1, 'the': 2, 'to': 3, 'a': 4, 'and': 5, 'of': 6, 'i': 7, 'in': 8, 'it': 9, 'url': 10, 'is': 11, 'that': 12, 'you': 13, 'for': 14, 'thi': 15, 'on': 16, 's': 17, 'be': 18, 'with': 19, 'have': 20, 'from': 21, 'not': 22, 'are': 23, 't': 24, 'as': 25, 'or': 26, 'your': 27, 'list': 28, 'if': 29, 'but': 30, 'at': 31, 'use': 32, 'can': 33, 'by': 34, 'all': 35, 'an': 36, 'my': 37, 'wa': 38, 'we': 39, 'get': 40, 'mail': 41, 'do': 42, 'they': 43, 'will': 44, 'so': 45, 'one': 46, 'there': 47, 'more': 48, 'ha': 49, 'time': 50, 'just': 51, 'about': 52, 'no': 53, 'what': 54, 'out': 55, 'like': 56, 'messag': 57, 'com': 58, 'up': 59, 'email': 60, 'onli': 61, 'which': 62, 'would': 63, 'work': 64, 'other': 65, 'make': 66, 'don': 67, 'some': 68, 'who': 69, 'ani': 70, 'me': 71, 'now': 72, 'when': 73, 'new': 74, 'peopl': 75, 'their': 76, 'our': 77, 'm': 78, 'free': 79, 'user': 80, 'been': 81, 'net': 82, 'date': 83, 'wrote': 84, 'how': 85, 'want': 86, 'than': 87, 'rpm': 88, 'them': 89, 'linux': 90, 'then': 91, 'also': 92, 'spamassassin': 93, 'file': 94, 'need': 95, 'he': 96, 'go': 97, 'year': 98, 'way': 99, 'think': 100, 'here': 101, 're': 102, 'becaus': 103, 'group': 104, 'into': 105, 'tri': 106, 'hi': 107, 'look': 108, 'know': 109, 'receiv': 110, 'should': 111, 'us': 112, 'had': 113, '_______________________________________________': 114, 'over': 115, 'run': 116, 'hyperlink': 117, 'e': 118, 'inform': 119, 've': 120, 'even': 121, 'd': 122, 'spam': 123, 'said': 124, 'could': 125, 'first': 126, 'most': 127, 'say': 128, 'see': 129, 'right': 130, 'chang': 131, 'problem': 132, 'were': 133, 'exmh': 134, 'pleas': 135, 'these': 136, 'world': 137, 'line': 138, 'day': 139, 'thing': 140, 'take': 141, 'may': 142, 'll': 143, 'money': 144, 'send': 145, 'system': 146, 'same': 147, 'where': 148, 'find': 149, 'good': 150, 'remov': 151, 'after': 152, 'much': 153, 'well': 154, 'name': 155, 'doe': 156, 'compani': 157, 'call': 158, 'those': 159, 'mani': 160, 'help': 161, 'veri': 162, 'talk': 163, 'servic': 164, 'subject': 165, 'am': 166, 'packag': 167, 'address': 168, 'includ': 169, 'start': 170, 'busi': 171, 'internet': 172, 'befor': 173, 'give': 174, 'state': 175, 'link': 176, 'set': 177, 'home': 178, 'own': 179, 'instal': 180, 'still': 181, 'someth': 182, 'through': 183, 'two': 184, 'come': 185, 'best': 186, 'gener': 187, 'differ': 188, 'too': 189, 'such': 190, 'realli': 191, 'site': 192, 'market': 193, 'seem': 194, 'phone': 195, 'write': 196, 'web': 197, 'comput': 198, 'c': 199, 'data': 200, 'unsubscrib': 201, 'softwar': 202, 'program': 203, 'read': 204, 'few': 205, 'person': 206, 'point': 207, 'check': 208, 'ie': 209, 'interest': 210, 'sure': 211, 'old': 212, 'found': 213, 'back': 214, 'base': 215, 'sep': 216, 'whi': 217, 'off': 218, 'report': 219, 'made': 220, 'million': 221, 'everi': 222, 'sponsor': 223, 'offer': 224, 'sinc': 225, 'each': 226, 'current': 227, 'last': 228, 'did': 229, 'server': 230, 'order': 231, 'case': 232, 'numbertnumb': 233, 'actual': 234, 'provid': 235, 'end': 236, 'today': 237, 'commun': 238, 'fork': 239, 'down': 240, 'follow': 241, 'secur': 242, 'origin': 243, 'mean': 244, 'next': 245, 'doesn': 246, 'test': 247, 'code': 248, 'anoth': 249, 'got': 250, 'perl': 251, 'r': 252, 'without': 253, 'long': 254, 'never': 255, 'put': 256, 'better': 257, 'razor': 258, 'anyon': 259, 'live': 260, 'real': 261, 'thank': 262, 'lot': 263, 'part': 264, 'place': 265, 'might': 266, 'sent': 267, 'manag': 268, 'onc': 269, 'month': 270, 'sourceforg': 271, 'while': 272, 'again': 273, 'x': 274, 'keep': 275, 'folder': 276, 'idea': 277, 'life': 278, 'ad': 279, 'network': 280, 'build': 281, 'error': 282, 'form': 283, 'week': 284, 'within': 285, 'version': 286, 'type': 287, 'requir': 288, 'invest': 289, 'probabl': 290, 'govern': 291, 'countri': 292, 'product': 293, 'show': 294, 'rate': 295, 'org': 296, 'yahoo': 297, 'support': 298, 'post': 299, 'issu': 300, 'discuss': 301, 'someon': 302, 'price': 303, 'click': 304, 'window': 305, 'mailto': 306, 'reason': 307, 'both': 308, 'sourc': 309, 'high': 310, 'releas': 311, 'suppli': 312, 'word': 313, 'custom': 314, 'develop': 315, 'easi': 316, 'must': 317, 'move': 318, 'redhat': 319, 'page': 320, 'let': 321, 'anyth': 322, 'result': 323, 'text': 324, 'etc': 325, 'less': 326, 'freshrpm': 327, 'past': 328, 'plan': 329, 'didn': 330, 'septemb': 331, 'around': 332, 'per': 333, 'content': 334, 'she': 335, 'news': 336, 'between': 337, 'total': 338, 'exampl': 339, 'train': 340, 'done': 341, 'ye': 342, 'allow': 343, 'great': 344, 'key': 345, 'f': 346, 'share': 347, 'though': 348, 'pay': 349, 'public': 350, 'wish': 351, 'tell': 352, 'matthia': 353, 'access': 354, 'maintain': 355, 'cours': 356, 'add': 357, 'sell': 358, 'hour': 359, 'happen': 360, 'her': 361, 'save': 362, 'kernel': 363, 'avail': 364, 'question': 365, 'open': 366, 'possibl': 367, 'ever': 368, 'under': 369, 'engin': 370, 'html': 371, 'log': 372, 'least': 373, 'instead': 374, 'power': 375, 'els': 376, 'header': 377, 'second': 378, 'red': 379, 'below': 380, 'littl': 381, 'buy': 382, 'account': 383, 'ask': 384, 'prefer': 385, 'believ': 386, 'creat': 387, 'rule': 388, 'stuff': 389, 'experi': 390, 'applic': 391, 'exist': 392, 'polit': 393, 'local': 394, 'recent': 395, 'futur': 396, 'updat': 397, 'success': 398, 'american': 399, 'process': 400, 'law': 401, 'mayb': 402, 'complet': 403, 'un': 404, 'friend': 405, 'import': 406, 'yet': 407, 'technolog': 408, 'fact': 409, 'enough': 410, 'against': 411, 'special': 412, 'howev': 413, 'w': 414, 'b': 415, 'thought': 416, 'bit': 417, 'contact': 418, 'deal': 419, 'becom': 420, 'l': 421, 'pm': 422, 'noth': 423, 'u': 424, 'stori': 425, 'select': 426, 'search': 427, 'big': 428, 'featur': 429, 'hand': 430, 'subscript': 431, 'far': 432, 'industri': 433, 'valu': 434, 'ham': 435, 'non': 436, 'larg': 437, 'alway': 438, 'client': 439, 'apt': 440, 'hat': 441, 'term': 442, 'alsa': 443, 'control': 444, 'offic': 445, 'p': 446, 'kind': 447, 'alreadi': 448, 'dollar': 449, 'thu': 450, 'learn': 451, 'cost': 452, 'trade': 453, 'protect': 454, 'onlin': 455, 'note': 456, 'stop': 457, 'isn': 458, 'play': 459, 'turn': 460, 'irish': 461, 'forward': 462, 'him': 463, 'n': 464, 'admin': 465, 'nation': 466, 'design': 467, 'thousand': 468, 'citi': 469, 'won': 470, 'book': 471, 'copi': 472, 'simpl': 473, 'understand': 474, 'hit': 475, 'feel': 476, 'love': 477, 'cd': 478, 'advertis': 479, 'respons': 480, '_': 481, 'effect': 482, 'directori': 483, 'record': 484, 'fix': 485, 'man': 486, 'aug': 487, 'score': 488, 'financi': 489, 'sound': 490, 'opportun': 491, 'seen': 492, 'either': 493, 'job': 494, 'act': 495, 'load': 496, 'ilug': 497, 'unit': 498, 'famili': 499, 'root': 500, 'increas': 501, 'dvd': 502, 'three': 503, 'mr': 504, 'legal': 505, 'card': 506, 'incom': 507, 'fund': 508, 'away': 509, 'databas': 510, 'box': 511, 'sequenc': 512, 'limit': 513, 'hope': 514, 'usr': 515, 'hard': 516, 'member': 517, 'appear': 518, 'claim': 519, 'full': 520, 'begin': 521, 'ok': 522, 'forc': 523, 'top': 524, 'corpor': 525, 'return': 526, 'answer': 527, 'true': 528, 'expect': 529, 'posit': 530, 'standard': 531, 'mind': 532, 'repli': 533, 'presid': 534, 'step': 535, 'rather': 536, 'join': 537, 'wrong': 538, 'connect': 539, 'level': 540, 'abl': 541, 'caus': 542, 'until': 543, 'g': 544, 'often': 545, 'care': 546, 'small': 547, 'direct': 548, 'bill': 549, 'drive': 550, 'rememb': 551, 'listmast': 552, 'j': 553, 'major': 554, 'machin': 555, 'signatur': 556, 'oper': 557, 'mark': 558, 'cell': 559, 'quit': 560, 'o': 561, 'specif': 562, 'partner': 563, 'regard': 564, 'chri': 565, 'ago': 566, 'profession': 567, 'perform': 568, 'accept': 569, 'notic': 570, 'size': 571, 'war': 572, 'forteana': 573, 'wed': 574, 'depend': 575, 'individu': 576, 'game': 577, 'wonder': 578, 'continu': 579, 'cash': 580, 'whether': 581, 'center': 582, 'bank': 583, 'project': 584, 'everyth': 585, 'bad': 586, 'sever': 587, 'univers': 588, 'devel': 589, 'sort': 590, 'abov': 591, 'contain': 592, 'welcom': 593, 'guarante': 594, 'area': 595, 'risk': 596, 'given': 597, 'copyright': 598, 'figur': 599, 'immedi': 600, 'dure': 601, 'bug': 602, 'parti': 603, 'polici': 604, 'further': 605, 'subscrib': 606, 'singl': 607, 'wait': 608, 'lost': 609, 'assist': 610, 'xent': 611, 'upon': 612, 'clean': 613, 'grow': 614, 'geek': 615, 'women': 616, 'digit': 617, 'request': 618, 'script': 619, 'lib': 620, 'sign': 621, 'anyway': 622, 'simpli': 623, 'comment': 624, 'intern': 625, 'quot': 626, 'half': 627, 'fail': 628, 'pretti': 629, 'entir': 630, 'econom': 631, 'info': 632, 'america': 633, 'fals': 634, 'tool': 635, 'whole': 636, 'meet': 637, 'execut': 638, 'privat': 639, 'imag': 640, 'option': 641, 'view': 642, 'decid': 643, 'java': 644, 'nice': 645, 'pick': 646, 'command': 647, 'upgrad': 648, 'matter': 649, 'later': 650, 'transact': 651, 'document': 652, 'ship': 653, 'worker': 654, 'msg': 655, 'egroup': 656, 'almost': 657, 'tire': 658, 'guy': 659, 'august': 660, 'detail': 661, 'h': 662, 'went': 663, 'john': 664, 'space': 665, 'solut': 666, 'addit': 667, 'similar': 668, 'via': 669, 'stock': 670, 'method': 671, 'agre': 672, 'suggest': 673, 'practic': 674, 'store': 675, 'transfer': 676, 'came': 677, 'final': 678, 'visit': 679, 'amount': 680, 'benefit': 681, 'except': 682, 'school': 683, 'fax': 684, 'attack': 685, 'fast': 686, 'agent': 687, 'websit': 688, 'discov': 689, 'easili': 690, 'research': 691, 'articl': 692, 'relat': 693, 'low': 694, 'minut': 695, 'global': 696, 'societi': 697, 'regist': 698, 'author': 699, 'exactli': 700, 'languag': 701, 'told': 702, 'interact': 703, 'although': 704, 'close': 705, 'modul': 706, 'sex': 707, 'instruct': 708, 'happi': 709, 'respect': 710, 'social': 711, 'watch': 712, 'distribut': 713, 'collect': 714, 'activ': 715, 'consid': 716, 'opt': 717, 'late': 718, 'memori': 719, 'profit': 720, 'driver': 721, 'publish': 722, 'domain': 723, 'feder': 724, 'python': 725, 'sa': 726, 'pass': 727, 'cut': 728, 'street': 729, 'lead': 730, 'daili': 731, 'left': 732, 'secret': 733, 'due': 734, 'mon': 735, 'hundr': 736, 'present': 737, 'bodi': 738, 'fall': 739, 'filter': 740, 'head': 741, 'associ': 742, 'bush': 743, 'everyon': 744, 'cv': 745, 'guess': 746, 'heaven': 747, 'assum': 748, 'involv': 749, 'monday': 750, 'propos': 751, 'spammer': 752, 'known': 753, 'face': 754, 'effort': 755, 'de': 756, 'reach': 757, 'die': 758, 'token': 759, 'improv': 760, 'perhap': 761, 'procmail': 762, 'basic': 763, 'appli': 764, 'worth': 765, 'insur': 766, 'leav': 767, 'natur': 768, 'remain': 769, 'clear': 770, 'miss': 771, 'fine': 772, 'choic': 773, 'explain': 774, 'normal': 775, 'car': 776, 'enabl': 777, 'tim': 778, 'fill': 779, 'hous': 780, 'googl': 781, 'offici': 782, 'action': 783, 'absolut': 784, 'inc': 785, 'tax': 786, 'deliv': 787, 'suit': 788, 'octob': 789, 'human': 790, 'men': 791, 'target': 792, 'osdn': 793, 'os': 794, 'credit': 795, 'devic': 796, 'platform': 797, 'thinkgeek': 798, 'charg': 799, 'averag': 800, 'paper': 801, 'beberg': 802, 'plu': 803, 'age': 804, 'download': 805, 'robert': 806, 'spend': 807, 'statement': 808, 'taint': 809, 'administr': 810, 'fri': 811, 'saou': 812, 'trust': 813, 'hettinga': 814, 'usa': 815, 'sale': 816, 'purchas': 817, 'delet': 818, 'main': 819, 'numberpm': 820, 'self': 821, 'unseen': 822, 'print': 823, 'ignor': 824, 'altern': 825, 'press': 826, 'port': 827, 'five': 828, 'rah': 829, 'electron': 830, 'suppos': 831, 'goe': 832, 'across': 833, 'contract': 834, 'speed': 835, 'along': 836, 'commiss': 837, 'particular': 838, 'integr': 839, 'refer': 840, 'cannot': 841, 'wasn': 842, 'paid': 843, 'york': 844, 'kill': 845, 'usual': 846, 'oct': 847, 'skip': 848, 'dave': 849, 'tag': 850, 'side': 851, 'hold': 852, 'track': 853, 'sun': 854, 'newslett': 855, 'organ': 856, 'studi': 857, 'capit': 858, 'folk': 859, 'night': 860, 'latest': 861, 'edit': 862, 'media': 863, 'regul': 864, 'commit': 865, 'potenti': 866, 'default': 867, 'display': 868, 'soon': 869, 'cc': 870, 'break': 871, 'hardwar': 872, 'lawrenc': 873, 'bitbitch': 874, 'compil': 875, 'adam': 876, 'letter': 877, 'william': 878, 'warn': 879, 'reserv': 880, 'object': 881, 'mass': 882, 'speak': 883, 'behalf': 884, 'abil': 885, 'male': 886, 'six': 887, 'unless': 888, 'concern': 889, 'haven': 890, 'hear': 891, 'invok': 892, 'earn': 893, 'lose': 894, 'yourself': 895, 'sens': 896, 'model': 897, 'mention': 898, 'implement': 899, 'co': 900, 'pictur': 901, 'tie': 902, 'longer': 903, 'pgp': 904, 'whatev': 905, 'util': 906, 'short': 907, 'earli': 908, 'configur': 909, 'gari': 910, 'itself': 911, 'id': 912, 'replac': 913, 'especi': 914, 'spec': 915, 'father': 916, 'third': 917, 'kid': 918, 'function': 919, 'procedur': 920, 'variou': 921, 'k': 922, 'scienc': 923, 'heard': 924, 'q': 925, 'advantag': 926, 'wouldn': 927, 'switch': 928, 'guid': 929, 'depart': 930, 'speech': 931, 'recommend': 932, 'insid': 933, 'produc': 934, 'indic': 935, 'numberth': 936, 'grant': 937, 'oh': 938, 'predict': 939, 'win': 940, 'qualiti': 941, 'togeth': 942, 'toni': 943, 'definit': 944, 'music': 945, 'appar': 946, 'washington': 947, 'button': 948, 'locat': 949, 'neg': 950, 'safe': 951, 'numberk': 952, 'aren': 953, 'took': 954, 'tuesday': 955, 'pc': 956, 'approach': 957, 'readi': 958, 'xml': 959, 'handl': 960, 'independ': 961, 'count': 962, 'rest': 963, 'attempt': 964, 'coupl': 965, 'valuabl': 966, 'format': 967, 'built': 968, 'popul': 969, 'certainli': 970, 'taken': 971, 'accord': 972, 'educ': 973, 'review': 974, 'section': 975, 'histori': 976, 'clue': 977, 'white': 978, 'spamd': 979, 'argument': 980, 'common': 981, 'dear': 982, 'consum': 983, 'modifi': 984, 'tom': 985, 'polic': 986, 'death': 987, 'michael': 988, 'mode': 989, 'quick': 990, 'rh': 991, 'disk': 992, 'south': 993, 'young': 994, 'deserv': 995, 'murphi': 996, 'bring': 997, 'path': 998, 'movi': 999, 'air': 1000}\n",
            "Sparse matrix shape: (2400, 1001)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spam_classifier.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Spam Email Classification with Logistic Regression\n",
        "\n",
        "This project builds a **spam detector** using raw email data from the SpamAssassin public corpus. The pipeline includes **data preprocessing, feature engineering, model training, and evaluation**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Dataset**\n",
        "\n",
        "* Downloaded ham (`easy_ham`) and spam (`spam`) emails.\n",
        "* Extracted from `.tar.bz2` archives.\n",
        "* Parsed emails with Python’s `email` module, preserving headers and MIME content.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Email Preprocessing**\n",
        "\n",
        "* Extract text from emails:\n",
        "\n",
        "  * Plain text: use directly.\n",
        "  * HTML: cleaned using `BeautifulSoup`, removed `<head>`, replaced links with `\"HYPERLINK\"`.\n",
        "* Normalize text:\n",
        "\n",
        "  * Lowercasing.\n",
        "  * Remove punctuation.\n",
        "  * Replace URLs with `\"URL\"`.\n",
        "  * Replace numbers with `\"NUMBER\"`.\n",
        "  * Apply stemming (Porter Stemmer).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Feature Engineering**\n",
        "\n",
        "* Convert emails → **word count dictionaries** (bag-of-words).\n",
        "* Build a **shared vocabulary** of most frequent words across all emails.\n",
        "* Convert word counts → **numerical vectors**:\n",
        "\n",
        "  * Each vector dimension represents a word in the vocabulary.\n",
        "  * Sparse representation used for efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Labels**\n",
        "\n",
        "* Ham emails → `0`\n",
        "* Spam emails → `1`\n",
        "* Split dataset: 80% training, 20% testing.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Model Training**\n",
        "\n",
        "* Logistic Regression classifier trained on the vectorized emails.\n",
        "* Model learns **weights for each word**:\n",
        "\n",
        "  * Positive weight → word indicative of spam.\n",
        "  * Negative weight → word indicative of ham.\n",
        "* Decision for new email:\n",
        "\n",
        "  1. Compute weighted sum of word counts + bias.\n",
        "  2. Apply sigmoid to get probability of spam.\n",
        "  3. Predict spam if probability > 0.5, otherwise ham.\n",
        "\n",
        "**Score formula:**\n",
        "\n",
        "score = SUM(word_count * word_weight) + bias\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **6. How the Model Handles Words**\n",
        "\n",
        "1. **Word Weights and Scoring**\n",
        "\n",
        "   * Each email vector is multiplied by learned word weights to compute a **spam score**.\n",
        "   * Words with higher positive weights push the score toward spam; negative weights push toward ham.\n",
        "   * Overlapping words (present in both ham and spam emails) get weights proportional to their correlation with spam.\n",
        "\n",
        "2. **Shared Vocabulary Across Ham and Spam**\n",
        "\n",
        "   * Only **one vocabulary** is used; not separate for ham and spam.\n",
        "   * Example:\n",
        "\n",
        "     * `\"meeting\"` mostly in ham → negative weight → reduces spam score.\n",
        "     * `\"free\"` mostly in spam → positive weight → increases spam score.\n",
        "   * Neutral or rare words get small weights → minimal effect.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Illustrative Example**\n",
        "\n",
        "Suppose our vocabulary has **4 words**:\n",
        " `[\"free\", \"meeting\", \"click\", \"project\"]`.\n",
        "\n",
        "| Word    | Weight (learned) |\n",
        "| ------- | ---------------- |\n",
        "| free    | +2.0             |\n",
        "| meeting | -1.5             |\n",
        "| click   | +1.8             |\n",
        "| project | -0.5             |\n",
        "\n",
        "#### **Email A (spam)**\n",
        "\n",
        "```\n",
        "\"Free click here!\"\n",
        "Word counts: {\"free\": 1, \"click\": 1}\n",
        "Score = 1*2.0 + 1*1.8 + 0*(-1.5) + 0*(-0.5) + bias\n",
        "      = 3.8 + bias\n",
        "P(spam) = sigmoid(3.8 + bias) ≈ high → classified as spam\n",
        "```\n",
        "\n",
        "#### **Email B (ham)**\n",
        "\n",
        "```\n",
        "\"Meeting about the project\"\n",
        "Word counts: {\"meeting\": 1, \"project\": 1}\n",
        "Score = 0*2.0 + 0*1.8 + 1*(-1.5) + 1*(-0.5) + bias\n",
        "      = -2.0 + bias\n",
        "P(spam) = sigmoid(-2.0 + bias) ≈ low → classified as ham\n",
        "```\n",
        "\n",
        "**Detailed Explanation of Email B Score:**\n",
        "\n",
        "1. **Word Counts:** Count how many times each vocabulary word occurs:\n",
        "\n",
        "   | Word    | Count |\n",
        "   | ------- | ----- |\n",
        "   | free    | 0     |\n",
        "   | click   | 0     |\n",
        "   | meeting | 1     |\n",
        "   | project | 1     |\n",
        "\n",
        "2. **Multiply by Learned Weights:**\n",
        "\n",
        "   ```\n",
        "   free: 0*2.0 = 0\n",
        "   click: 0*1.8 = 0\n",
        "   meeting: 1*(-1.5) = -1.5\n",
        "   project: 1*(-0.5) = -0.5\n",
        "   ```\n",
        "\n",
        "3. **Add Bias:**\n",
        "\n",
        "   ```\n",
        "   Score = -1.5 + (-0.5) + bias = -2.0 + bias\n",
        "   ```\n",
        "\n",
        "4. **Convert to Probability:**\n",
        "\n",
        "   ```\n",
        "   P(spam) = 1 / (1 + exp(-score)) ≈ low\n",
        "   ```\n",
        "\n",
        "5. **Prediction:** Low probability → classified as ham.\n",
        "\n",
        "**Key points demonstrated:**\n",
        "\n",
        "* Words shared in both ham and spam (“project”) influence the score based on their **learned weight**.\n",
        "* Logistic regression combines all word contributions to compute the final spam probability.\n",
        "* Each word’s contribution = `count * weight`. Bias shifts overall probability to improve accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Evaluation**\n",
        "\n",
        "* Cross-validation on training set to estimate performance.\n",
        "* Tested on unseen test emails.\n",
        "* Metrics reported:\n",
        "\n",
        "  * **Precision:** % of predicted spam that is actually spam.\n",
        "  * **Recall:** % of actual spam correctly detected.\n",
        "\n",
        "---\n",
        "\n",
        "**Outcome:** A classical ML spam classifier using **bag-of-words features**, logistic regression, and shared vocabulary with learned word weights, capable of distinguishing ham and spam based on word patterns, with **score computation formula clearly explained**.\n"
      ],
      "metadata": {
        "id": "2NvxJErvLotj"
      }
    }
  ]
}